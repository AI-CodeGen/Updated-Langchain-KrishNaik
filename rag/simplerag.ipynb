{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic dependencies and imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion - 3 Ways shown below\n",
    "1. Text file\n",
    "2. Web Scraping\n",
    "3. PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Ingestion - Text File Loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents  # To see the content of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion - Web Based Loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# load,chunk and index the content of the html page\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           # class names are the class of the div of the html page, which was found by inspecting the page\n",
    "                           class_=(\"post-title\", \"post-content\", \"post-header\")\n",
    "                       )))\n",
    "\n",
    "text_documents = loader.load()\n",
    "text_documents  # To see the content of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Ingestion - PDF File Loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('attention.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation - Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)    # Normal implementation\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=150)  # Attempt 1 - To increase search accuracy percentage by reducing the chunk size\n",
    "\n",
    "#NOTE : Reducing the chunk size from 1000 and reducing the chunk overlap from 200 to 150, increased the search accuracy percentage from 59.0010% to 62.6558 %\n",
    "\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]\n",
    "#documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embedding and Vector Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI Approach 1 - Vector Embedding And Vector Store - Using OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings   # For embedding the text using OpenAI\n",
    "from langchain_community.vectorstores import Chroma # Chroma is a vector db store, provided by langchain_community\n",
    "\n",
    "#TODO : Research more on various embedding models and vector stores\n",
    "\n",
    "db = Chroma.from_documents(documents,OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ollama Approach 1 - Vector Embedding And Vector Store - Using Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings   # For embedding the text using Ollama\n",
    "from langchain_community.vectorstores import Chroma # Chroma is a vector db store, provided by langchain_community\n",
    "\n",
    "db = Chroma.from_documents(documents,OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ollama Approach 2 - Vector Embedding And Vector Store - Using Ollama and ecqulidian distance\n",
    "import os\n",
    "import chromadb\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Create a persistent directory\n",
    "persist_directory = \"chroma_db\"\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "\n",
    "# Configure Chroma settings\n",
    "client_settings = chromadb.config.Settings(\n",
    "    is_persistent=True,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# Initialize Chroma with Ollama embeddings and persistence\n",
    "db = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=OllamaEmbeddings(),\n",
    "    persist_directory=persist_directory,\n",
    "    client_settings=client_settings,\n",
    "    collection_name=\"attention_paper\",  # Give your collection a name\n",
    ")\n",
    "\n",
    "# Persist the database\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing collection attention_paper_cosine deleted\n",
      "Collection attention_paper_cosine saved to chroma_db_cosine with cosine distance function.\n"
     ]
    }
   ],
   "source": [
    "## Ollama Approach 3 - Vector Embedding And Vector Store - Using Ollama and cosine distance\n",
    "import os\n",
    "import chromadb\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Create a persistent directory and collection name\n",
    "persist_directory = \"chroma_db_cosine\"\n",
    "collection_name = \"attention_paper_cosine\"\n",
    "\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "\n",
    "# Configure Chroma settings\n",
    "client_settings = chromadb.config.Settings(\n",
    "    is_persistent=True,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# Create Chroma client\n",
    "client = chromadb.Client(client_settings)\n",
    "\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "    print(f\"Existing collection {collection_name} deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing collection {collection_name} to delete: {e}\")\n",
    "\n",
    "# Create collection with specific distance function\n",
    "#collection = client.get_or_create_collection(\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Available options: \"cosine\", \"l2\", \"ip\"\n",
    ")\n",
    "\n",
    "# Initialize Chroma with Ollama embeddings and persistence\n",
    "db_cosine = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=OllamaEmbeddings(),\n",
    "    persist_directory=persist_directory,\n",
    "    client_settings=client_settings,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "# Persist the database\n",
    "db_cosine.persist()\n",
    "print(f\"Collection {collection_name} saved to {persist_directory} with cosine distance function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to faiss_db/attention_paper_index\n"
     ]
    }
   ],
   "source": [
    "## FAISS Approach 1 - Vector Embedding And Vector Store - FAISS Vector Database\n",
    "from langchain_community.vectorstores import FAISS  # FAISS is a vector db store, provided by facebook\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create a persistent directory and collection name\n",
    "persist_directory = \"faiss_db\"\n",
    "faiss_index_name = \"attention_paper_index\"\n",
    "index_path = os.path.join(persist_directory, faiss_index_name)\n",
    "\n",
    "# Check and delete existing index\n",
    "if os.path.exists(index_path):\n",
    "    try:\n",
    "        shutil.rmtree(index_path)\n",
    "        print(f\"Existing FAISS index at {index_path} deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting existing index: {e}\")\n",
    "else:\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory)\n",
    "        print(f\"Created new directory: {persist_directory}\")\n",
    "    \n",
    "# Use either of the below(Comment out the one not in use):\n",
    "## For Embeddings using OpenAI( - Paid)\n",
    "#db_faiss = FAISS.from_documents(documents[:15], OpenAIEmbeddings())\n",
    "\n",
    "## For Embeddings using Ollama( - Free as its local)\n",
    "db_faiss = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=OllamaEmbeddings()\n",
    ")\n",
    "\n",
    "# Save the FAISS index\n",
    "db_faiss.save_local(\n",
    "    folder_path=persist_directory,\n",
    "    index_name=faiss_index_name\n",
    ")\n",
    "print(f\"FAISS index saved to {os.path.join(persist_directory, faiss_index_name)}\")\n",
    "\n",
    "# To load the saved index later:\n",
    "db_faiss = FAISS.load_local(\n",
    "    folder_path=persist_directory,\n",
    "    index_name=faiss_index_name,\n",
    "    embeddings=OllamaEmbeddings(),\n",
    "    allow_dangerous_deserialization=True  # Only use if you trust the source of the index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Querying - From Vector Store(DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 1 - Chroma DB - Simple similarity search - query\n",
    "query = \"Who are the authors of attention is all you need?\"\n",
    "\n",
    "#TODO : Research more on various search methods\n",
    "\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relevance Score: 62.6558 %\n",
      "Content: In International Conference on Learning Representations, 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "Metadata: {'source': 'attention.pdf', 'page': 10}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevance Score: 56.8199 %\n",
      "Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Metadata: {'page': 12, 'source': 'attention.pdf'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevance Score: 51.3391 %\n",
      "Content: Language Processing, pages 832–841. ACL, August 2009.\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS), 2016.\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR), 2016.\n",
      "Metadata: {'source': 'attention.pdf', 'page': 10}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Approach 2 - Chroma DB - Similarity search with relevance scores - query\n",
    "query = \"Who are the authors of attention is all you need?\"\n",
    "\n",
    "# Fetch top 3 results for the query\n",
    "results = db_cosine.similarity_search_with_relevance_scores(query, k=3)\n",
    "for doc, score in results:\n",
    "    print(f\"\\nRelevance Score: {score * 100:.4f} %\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Basic Similarity Search with Scores - L2 distance\n",
      "--------------------------------------------------\n",
      "\n",
      "Similarity: -1070487.89%\n",
      "Content: In International Conference on Learning Representations, 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. F...\n",
      "Metadata: {'source': 'attention.pdf', 'page': 10}\n",
      "--------------------------------------------------\n",
      "\n",
      "Similarity: -1234675.39%\n",
      "Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "Metadata: {'source': 'attention.pdf', 'page': 12}\n",
      "--------------------------------------------------\n",
      "\n",
      "Similarity: -1361067.68%\n",
      "Content: Language Processing, pages 832–841. ACL, August 2009.\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv...\n",
      "Metadata: {'source': 'attention.pdf', 'page': 10}\n",
      "--------------------------------------------------\n",
      "Method 1: Basic Similarity Search with Scores - cosine similarity\n",
      "--------------------------------------------------\n",
      "\n",
      "Similarity: -1070487.89%\n",
      "Content: In International Conference on Learning Representations, 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. F...\n",
      "Metadata: {'source': 'attention.pdf', 'page': 10}\n",
      "--------------------------------------------------\n",
      "\n",
      "Similarity: -1234675.39%\n",
      "Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "Metadata: {'source': 'attention.pdf', 'page': 12}\n",
      "--------------------------------------------------\n",
      "\n",
      "Similarity: -1361067.68%\n",
      "Content: Language Processing, pages 832–841. ACL, August 2009.\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv...\n",
      "Metadata: {'source': 'attention.pdf', 'page': 10}\n",
      "--------------------------------------------------\n",
      "\n",
      "Method 2: MMR Search (Diverse Results)\n",
      "--------------------------------------------------\n",
      "\n",
      "Similarity: -23803.32%\n",
      "Content: In International Conference on Learning Representations, 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. F...\n",
      "Metadata: {'source': 'attention.pdf', 'page': 10}\n",
      "Diversity Factor (lambda_mult): 0.7\n",
      "--------------------------------------------------\n",
      "\n",
      "Similarity: -46940.69%\n",
      "Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "Metadata: {'source': 'attention.pdf', 'page': 12}\n",
      "Diversity Factor (lambda_mult): 0.7\n",
      "--------------------------------------------------\n",
      "\n",
      "Similarity: -55412.48%\n",
      "Content: where headi = Attention(QWQ\n",
      "i , KWK\n",
      "i , V WV\n",
      "i )\n",
      "Where the projections are parameter matricesWQ\n",
      "i ∈ Rdmodel×dk , WK\n",
      "i ∈ Rdmodel×dk , WV\n",
      "i ∈ Rdmodel×dv\n",
      "and WO ∈ Rhdv×dmodel .\n",
      "In this work we employ h =...\n",
      "Metadata: {'source': 'attention.pdf', 'page': 4}\n",
      "Diversity Factor (lambda_mult): 0.7\n",
      "--------------------------------------------------\n",
      "\n",
      "Method 3: Filtered Similarity Search\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Approach 3 - FAISS DB - Various Similarity search approaches\n",
    "query = \"Who are the authors of attention is all you need?\"\n",
    "\n",
    "# Helper function to normalize scores\n",
    "def normalize_score(score):\n",
    "    \"\"\"Convert FAISS distance score to percentage similarity.\"\"\"\n",
    "    # FAISS returns L2 distance, smaller is better\n",
    "    # Max reasonable L2 distance is around 2.0 for normalized vectors\n",
    "    max_l2_distance = 2.0\n",
    "    similarity = max(0, min(100, (1 - (score / max_l2_distance)) * 100))\n",
    "    return similarity\n",
    "\n",
    "# Method 1: Basic similarity search with score threshold\n",
    "results_with_scores = db_faiss.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=3  # Get top 3 results\n",
    ")\n",
    "\n",
    "results_with_scores_in_cosine = db_faiss.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=3,\n",
    "    metric=\"cosine\"  # Use cosine similarity instead of L2\n",
    ")\n",
    "\n",
    "print(\"Method 1: Basic Similarity Search with Scores - L2 distance\")\n",
    "print(\"-\" * 50)\n",
    "for doc, score in results_with_scores:\n",
    "    # Lower score means more similar in FAISS\n",
    "    similarity_percentage = (1 - score) * 100\n",
    "    print(f\"\\nSimilarity: {similarity_percentage:.2f}%\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "print(\"Method 1: Basic Similarity Search with Scores - cosine similarity\")\n",
    "print(\"-\" * 50)\n",
    "for doc, score in results_with_scores_in_cosine:\n",
    "    # Lower score means more similar in FAISS\n",
    "    similarity_percentage = (1 - score) * 100\n",
    "    print(f\"\\nSimilarity: {similarity_percentage:.2f}%\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Method 2: MMR Search for diverse results\n",
    "mmr_results = db_faiss.max_marginal_relevance_search(\n",
    "    query=query,\n",
    "    k=3,  # Number of documents to return\n",
    "    fetch_k=10,  # Number of documents to fetch before filtering\n",
    "    lambda_mult=0.7  # Diversity factor (0=max diversity, 1=max similarity)\n",
    ")\n",
    "\n",
    "print(\"\\nMethod 2: MMR Search (Diverse Results)\")\n",
    "print(\"-\" * 50)\n",
    "\"\"\" for i, doc in enumerate(mmr_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50) \"\"\"\n",
    "# Get similarity scores for MMR results\n",
    "for doc in mmr_results:\n",
    "    # Get similarity score for this document\n",
    "    score = db_faiss.similarity_search_with_score(\n",
    "        doc.page_content,  # Use document content as query\n",
    "        k=1  # Get only the closest match\n",
    "    )[0][1]  # Extract the score from the result\n",
    "    \n",
    "    # Convert score to similarity percentage\n",
    "    similarity_percentage = (1 - score) * 100\n",
    "    #similarity_percentage = normalize_score(score)\n",
    "    \n",
    "    print(f\"\\nSimilarity: {similarity_percentage:.2f}%\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Diversity Factor (lambda_mult): 0.7\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Method 3: Hybrid search with metadata filtering\n",
    "# Assuming documents have 'section' metadata\n",
    "filtered_results = db_faiss.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=3,\n",
    "    filter={\"section\": \"abstract\"}  # Optional: filter by metadata if available\n",
    ")\n",
    "\n",
    "print(\"\\nMethod 3: Filtered Similarity Search\")\n",
    "print(\"-\" * 50)\n",
    "for doc, score in filtered_results:\n",
    "    similarity_percentage = (1 - score) * 100\n",
    "    print(f\"\\nSimilarity: {similarity_percentage:.2f}%\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced RAG search - Chain and Retreival using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The retrival methods used above are not that accurate hence we will use LLM chain to get the more accurate results\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load the Ollama model\n",
    "llm = Ollama(model=\"llama2\", temperature=0.7)  # Adjust temperature for creativity\n",
    "\n",
    "# Define the chat prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on provided context.\n",
    "    Think step by step before providing a detailed answer.\n",
    "    <context> {context} <context/>\n",
    "    Question: {input}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stuff Document Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10ba3fa70>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the retriever(an interface to a vector store which has all the information)\n",
    "\n",
    "# Vector store retriever for FAISS db\n",
    "retriever = db_faiss.as_retriever(search_kwargs={\"k\": 5})  # Fetch top 5 documents\n",
    "#retriever = db_faiss.as_retriever()  # Fetch from all documents\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10ba3fa70>, search_kwargs={'k': 5}), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='Answer the following question based only on provided context.\\n    Think step by step before providing a detailed answer.\\n    <context> {context} <context/>\\n    Question: {input}'))])\n",
       "            | Ollama(temperature=0.7)\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, creating the retriever chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever,chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "How much time did each base model training step took?\n",
      "Answer:\n",
      "Based on the context provided, we can infer that the author is asking for the time taken by each base model training step.\n",
      "\n",
      "From the text, we know that there were 8 P100 GPUs used for training, and the training took 3.5 days. We can also see that the author mentions that the configuration of the model is listed in the bottom line of Table 3.\n",
      "\n",
      "Therefore, we can calculate the time taken by each base model training step as follows:\n",
      "\n",
      "Time taken by each base model training step = 3.5 days / 8 GPUs = 0.4425 days (or approximately 1 hour and 27 minutes) per training step.\n"
     ]
    }
   ],
   "source": [
    "# Now, query the chain with a question\n",
    "\n",
    "query1 = \"Who are the authors of attention is all you need?\"\n",
    "query2 = \"What is a scaled dot product attention?\"\n",
    "query3 = \"What are the types of attention?\"\n",
    "query4 = \"How much time did each base model training step took?\"\n",
    "\n",
    "# response = retrieval_chain.invoke({\"input\": query1})\n",
    "# response = retrieval_chain.invoke({\"input\": query2})\n",
    "\n",
    "# INTERESTING : As we have limited the number of documents to 5 by search_kwargs={\"k\": 5}, it is not able to find the correct number of items to list.\n",
    "# BUT, if we remove the search_kwargs={\"k\": 5} it does not nessecarily return the correct number of items\n",
    "# response = retrieval_chain.invoke({\"input\": query3})\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": query4})\n",
    "\n",
    "print(\"Question:\")\n",
    "print(response['input'])\n",
    "# print(\"Context Used:\")\n",
    "# print(response['context'])\n",
    "print(\"Answer:\")\n",
    "# To the point answer, BUT CAN CHANGE WITH EACH RUN, and hence accuracy is not guaranteed\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
